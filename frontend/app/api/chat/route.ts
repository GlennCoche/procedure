import { NextRequest } from 'next/server'
import { db } from '@/lib/db'
import { getCurrentUser } from '@/lib/auth'
import OpenAI from 'openai'

// Cr√©er le client OpenAI de mani√®re lazy pour √©viter les erreurs au build
function getOpenAIClient() {
  const apiKey = process.env.OPENAI_API_KEY
  if (!apiKey) {
    throw new Error('OPENAI_API_KEY is not configured')
  }
  return new OpenAI({ apiKey })
}

// Prompts selon le mode
const EXPERT_PROMPT_STANDARD = `Tu es un EXPERT SENIOR en maintenance photovolta√Øque avec 25 ans d'exp√©rience terrain.

COMPORTEMENT OBLIGATOIRE:

1. CLARIFICATION D'ABORD
   - Si la question est ambigu√´ ou manque de d√©tails, demande des pr√©cisions AVANT de r√©pondre
   - Demande: marque de l'√©quipement, mod√®le exact, code erreur affich√©, contexte d'intervention
   - Exemple: "Pour mieux vous aider, pouvez-vous pr√©ciser le mod√®le exact de l'onduleur et le message d'erreur affich√© ?"

2. BASE DE DONN√âES EN PRIORIT√â
   - Utilise TOUJOURS les proc√©dures et tips de la base en priorit√©
   - Cite explicitement tes sources: "Selon la proc√©dure 'Installation ABB TRIO'..."

3. R√âPONSE STRUCTUR√âE (format obligatoire)
   üìã DIAGNOSTIC
   R√©sum√© du probl√®me tel que tu l'as compris.

   ‚úÖ SOLUTION PRINCIPALE  
   √âtapes d√©taill√©es depuis la base documentaire si disponible.

   üîÑ ALTERNATIVES
   Autres approches possibles si la solution principale ne fonctionne pas.

   ‚ö†Ô∏è PR√âCAUTIONS
   Points de s√©curit√© et mises en garde importantes.

   üìö R√âF√âRENCES
   Proc√©dures et tips pertinents de la base (avec titres exacts).

4. SP√âCIFICIT√âS FRANCE
   - Connais les normes NF C 15-100, UTE C 15-712
   - Standards r√©seau France: 230/400V, 50Hz

CONTEXTE TECHNIQUE DISPONIBLE:
{context}

R√©ponds en fran√ßais, de mani√®re professionnelle mais accessible.`

const EXPERT_PROMPT_CONCISE = `Tu es un EXPERT SENIOR en maintenance photovolta√Øque. R√©ponds de mani√®re CONCISE et PR√âCISE.

R√àGLES STRICTES:
1. R√©ponses COURTES: 3-5 phrases maximum par section
2. Va DROIT AU BUT: pas de pr√©ambule, pas de redondance
3. POSE DES QUESTIONS si besoin de pr√©cisions (max 2 questions cibl√©es)
4. Format bullet points quand possible
5. Cite uniquement les r√©f√©rences essentielles

FORMAT DE R√âPONSE:
‚Ä¢ DIAGNOSTIC: 1-2 phrases
‚Ä¢ SOLUTION: √âtapes num√©rot√©es, concises
‚Ä¢ ‚ö†Ô∏è S√âCURIT√â: Points critiques uniquement
‚Ä¢ ‚ùì QUESTIONS: Si besoin de pr√©cisions

{context}

R√©ponds en fran√ßais. Sois direct et efficace.`

export async function POST(request: NextRequest) {
  const user = await getCurrentUser()
  if (!user) {
    return new Response('Non authentifi√©', { status: 401 })
  }

  try {
    const { message, context, settings } = await request.json()

    if (!message) {
      return new Response('Message requis', { status: 400 })
    }

    // Param√®tres de configuration
    const conciseMode = settings?.concise ?? false
    const dualMode = settings?.dualResponse ?? false

    // R√©cup√©rer l'historique des messages
    const history = await db.chatMessage.findMany({
      where: { userId: user.id },
      orderBy: { createdAt: 'desc' },
      take: 10,
    })

    // AUTO-LEARNING: R√©cup√©rer les r√©ponses bien not√©es
    let learningContext = ''
    try {
      const positiveExamples = await db.messageRating.findMany({
        where: {
          rating: 'positive',
          message: { userId: user.id },
        },
        include: {
          message: { select: { message: true, response: true } },
        },
        orderBy: { createdAt: 'desc' },
        take: 3,
      })

      if (positiveExamples.length > 0) {
        learningContext = '\n\nüìä STYLE APPR√âCI√â:\n'
        for (const ex of positiveExamples) {
          if (ex.message.response) {
            learningContext += `‚Ä¢ "${ex.message.response.slice(0, 150)}..."\n`
          }
        }
      }
    } catch {
      // Table ratings n'existe pas encore
    }

    // Construire le contexte depuis la base de donn√©es
    let contextInfo = ''
    try {
      const keywords = message.toLowerCase().split(/\s+/).filter((w: string) => w.length > 3).slice(0, 5)
      
      if (keywords.length > 0) {
        const procedures = await db.procedure.findMany({
          where: {
            OR: keywords.flatMap((keyword: string) => [
              { title: { contains: keyword, mode: 'insensitive' as const } },
              { description: { contains: keyword, mode: 'insensitive' as const } },
            ])
          },
          include: { steps: { orderBy: { order: 'asc' } } },
          take: 3
        })

        if (procedures.length > 0) {
          contextInfo += '\nüìñ PROC√âDURES:\n'
          for (const proc of procedures) {
            contextInfo += `‚Ä¢ "${proc.title}": ${proc.steps.map(s => s.title).join(' ‚Üí ')}\n`
          }
        }

        const tips = await db.tip.findMany({
          where: {
            OR: keywords.flatMap((keyword: string) => [
              { title: { contains: keyword, mode: 'insensitive' as const } },
              { content: { contains: keyword, mode: 'insensitive' as const } },
            ])
          },
          take: 3
        })

        if (tips.length > 0) {
          contextInfo += '\nüí° TIPS:\n'
          for (const tip of tips) {
            contextInfo += `‚Ä¢ "${tip.title}": ${tip.content.slice(0, 100)}...\n`
          }
        }
      }
    } catch (error) {
      console.warn('Recherche par mots-cl√©s √©chou√©e:', error)
    }

    if (!contextInfo) {
      contextInfo = '\n‚ö†Ô∏è Aucune doc trouv√©e. Utilise tes connaissances expert.\n'
    }

    const fullContext = contextInfo + learningContext
    const basePrompt = conciseMode ? EXPERT_PROMPT_CONCISE : EXPERT_PROMPT_STANDARD
    const systemMessage = basePrompt.replace('{context}', fullContext)

    // Construire les messages pour OpenAI
    const openaiMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
      { role: 'system', content: systemMessage },
      ...history
        .reverse()
        .filter((msg) => msg.message && msg.response)
        .slice(-6)
        .flatMap((msg) => [
          { role: 'user' as const, content: msg.message },
          { role: 'assistant' as const, content: msg.response! },
        ]),
      { role: 'user', content: message },
    ]

    const openai = getOpenAIClient()

    // MODE DUAL: G√©n√©rer 2 r√©ponses alternatives
    if (dualMode) {
      const [response1, response2] = await Promise.all([
        openai.chat.completions.create({
          model: 'gpt-4o-mini',
          messages: openaiMessages,
          temperature: 0.3,
          max_tokens: conciseMode ? 500 : 1500,
        }),
        openai.chat.completions.create({
          model: 'gpt-4o-mini',
          messages: [
            ...openaiMessages.slice(0, -1),
            { 
              role: 'user', 
              content: message + '\n\n[Propose une approche ALTERNATIVE diff√©rente de la premi√®re qui pourrait venir √† l\'esprit]' 
            }
          ],
          temperature: 0.7,
          max_tokens: conciseMode ? 500 : 1500,
        }),
      ])

      const content1 = response1.choices[0]?.message?.content || ''
      const content2 = response2.choices[0]?.message?.content || ''

      // Sauvegarder le message (sans r√©ponse pour l'instant)
      const chatMessage = await db.chatMessage.create({
        data: {
          userId: user.id,
          message,
          context: JSON.stringify({ ...context, dualMode: true }),
        },
      })

      return new Response(JSON.stringify({
        messageId: chatMessage.id,
        dualMode: true,
        responses: [
          { id: 'A', content: content1, label: 'R√©ponse A - Approche standard' },
          { id: 'B', content: content2, label: 'R√©ponse B - Approche alternative' },
        ]
      }), {
        headers: { 'Content-Type': 'application/json' },
      })
    }

    // MODE STANDARD: Streaming
    const chatMessage = await db.chatMessage.create({
      data: {
        userId: user.id,
        message,
        context: context ? JSON.stringify(context) : null,
      },
    })

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: openaiMessages,
      stream: true,
      temperature: conciseMode ? 0.3 : 0.5,
      max_tokens: conciseMode ? 600 : 2000,
    })

    const stream = new ReadableStream({
      async start(controller) {
        let fullResponse = ''

        try {
          controller.enqueue(
            new TextEncoder().encode(`data: ${JSON.stringify({ messageId: chatMessage.id })}\n\n`)
          )

          for await (const chunk of completion) {
            const content = chunk.choices[0]?.delta?.content || ''
            if (content) {
              fullResponse += content
              controller.enqueue(
                new TextEncoder().encode(`data: ${JSON.stringify({ content })}\n\n`)
              )
            }
          }

          controller.enqueue(new TextEncoder().encode('data: [DONE]\n\n'))

          await db.chatMessage.update({
            where: { id: chatMessage.id },
            data: { response: fullResponse },
          })

          controller.close()
        } catch (error) {
          console.error('Erreur streaming:', error)
          controller.enqueue(
            new TextEncoder().encode(`data: ${JSON.stringify({ error: 'Erreur lors de la g√©n√©ration' })}\n\n`)
          )
          controller.close()
        }
      },
    })

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        Connection: 'keep-alive',
      },
    })
  } catch (error) {
    console.error('Erreur chat:', error)
    return new Response('Erreur serveur', { status: 500 })
  }
}

// Endpoint pour s√©lectionner une r√©ponse en mode dual
export async function PUT(request: NextRequest) {
  const user = await getCurrentUser()
  if (!user) {
    return new Response('Non authentifi√©', { status: 401 })
  }

  try {
    const { messageId, selectedResponse, selectedId } = await request.json()

    if (!messageId || !selectedResponse) {
      return new Response('Donn√©es manquantes', { status: 400 })
    }

    // Mettre √† jour le message avec la r√©ponse s√©lectionn√©e
    await db.chatMessage.update({
      where: { id: messageId },
      data: { 
        response: selectedResponse,
        context: JSON.stringify({ selectedChoice: selectedId })
      },
    })

    // Enregistrer automatiquement un feedback positif pour la r√©ponse choisie
    try {
      await db.messageRating.create({
        data: {
          messageId,
          rating: 'positive',
          feedback: `R√©ponse ${selectedId} s√©lectionn√©e`,
        },
      })
    } catch {
      // Table ratings n'existe pas encore
    }

    return new Response(JSON.stringify({ success: true }), {
      headers: { 'Content-Type': 'application/json' },
    })
  } catch (error) {
    console.error('Erreur s√©lection r√©ponse:', error)
    return new Response('Erreur serveur', { status: 500 })
  }
}

import { NextRequest } from 'next/server'
import { db } from '@/lib/db'
import { getCurrentUser } from '@/lib/auth'
import OpenAI from 'openai'
import { Prisma } from '@prisma/client'

// Cr√©er le client OpenAI de mani√®re lazy pour √©viter les erreurs au build
function getOpenAIClient() {
  const apiKey = process.env.OPENAI_API_KEY
  if (!apiKey) {
    throw new Error('OPENAI_API_KEY is not configured')
  }
  return new OpenAI({ apiKey })
}

// System prompt expert photovolta√Øque
const EXPERT_SYSTEM_PROMPT = `Tu es un EXPERT SENIOR en maintenance photovolta√Øque avec 25 ans d'exp√©rience terrain.

COMPORTEMENT OBLIGATOIRE:

1. CLARIFICATION D'ABORD
   - Si la question est ambigu√´ ou manque de d√©tails, demande des pr√©cisions AVANT de r√©pondre
   - Demande: marque de l'√©quipement, mod√®le exact, code erreur affich√©, contexte d'intervention
   - Exemple: "Pour mieux vous aider, pouvez-vous pr√©ciser le mod√®le exact de l'onduleur et le message d'erreur affich√© ?"

2. BASE DE DONN√âES EN PRIORIT√â
   - Utilise TOUJOURS les proc√©dures et tips de la base en priorit√©
   - Cite explicitement tes sources: "Selon la proc√©dure 'Installation ABB TRIO'..."
   - Indique clairement si l'info vient de la base documentaire ou de tes connaissances g√©n√©rales

3. R√âPONSE STRUCTUR√âE (format obligatoire)
   üìã DIAGNOSTIC
   R√©sum√© du probl√®me tel que tu l'as compris.

   ‚úÖ SOLUTION PRINCIPALE  
   √âtapes d√©taill√©es depuis la base documentaire si disponible.

   üîÑ ALTERNATIVES
   Autres approches possibles si la solution principale ne fonctionne pas.

   ‚ö†Ô∏è PR√âCAUTIONS
   Points de s√©curit√© et mises en garde importantes.

   üìö R√âF√âRENCES
   Proc√©dures et tips pertinents de la base (avec titres exacts).

4. SI INFORMATION MANQUANTE
   - Indique clairement que l'info n'est pas dans la base documentaire
   - Donne une r√©ponse bas√©e sur tes connaissances d'expert
   - Propose des pistes de recherche sur les sites constructeurs officiels

5. STYLE EXPERT
   - Langage technique pr√©cis mais accessible
   - Valeurs num√©riques quand pertinent (tensions, courants, temp√©ratures)
   - Conseils terrain bas√©s sur l'exp√©rience pratique
   - Mise en garde s√©curit√© SYST√âMATIQUE (risques √©lectriques, travail en hauteur)

6. SP√âCIFICIT√âS FRANCE
   - Connais les normes NF C 15-100, UTE C 15-712
   - Standards r√©seau France: 230/400V, 50Hz
   - R√©f√©rences aux seuils de d√©clenchement standard France

CONTEXTE TECHNIQUE DISPONIBLE:
{context}

R√©ponds en fran√ßais, de mani√®re professionnelle mais accessible.`

export async function POST(request: NextRequest) {
  const user = await getCurrentUser()
  if (!user) {
    return new Response('Non authentifi√©', { status: 401 })
  }

  try {
    const { message, context } = await request.json()

    if (!message) {
      return new Response('Message requis', { status: 400 })
    }

    // R√©cup√©rer l'historique des messages (augment√© √† 15)
    const history = await db.chatMessage.findMany({
      where: { userId: user.id },
      orderBy: { createdAt: 'desc' },
      take: 15,
    })

    // Construire le contexte depuis la base de donn√©es
    let contextInfo = ''
    let foundProcedures: string[] = []
    let foundTips: string[] = []

    // Recherche vectorielle pour enrichir le contexte
    try {
      const openai = getOpenAIClient()
      const embeddingResponse = await openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: message.slice(0, 8000),
      })
      const queryEmbedding = embeddingResponse.data[0].embedding
      const embeddingStr = '[' + queryEmbedding.join(',') + ']'

      // Recherche vectorielle am√©lior√©e (seuil abaiss√© √† 0.5, top_k augment√© √† 10)
      const escapedEmbedding = embeddingStr.replace(/'/g, "''")
      const vectorResults = await db.$queryRaw<Array<{
        id: number
        document_type: string
        document_id: number
        content: string
        metadata: string | null
        similarity: number
      }>>(
        Prisma.raw(`
          SELECT 
            id,
            document_type,
            document_id,
            content,
            metadata,
            1 - (embedding <=> '${escapedEmbedding}'::vector) as similarity
          FROM document_embeddings
          WHERE embedding IS NOT NULL
          AND (1 - (embedding <=> '${escapedEmbedding}'::vector)) >= 0.5
          ORDER BY similarity DESC
          LIMIT 10
        `)
      )

      // Enrichir le contexte avec les r√©sultats de recherche
      if (vectorResults.length > 0) {
        contextInfo += '\nüìñ DOCUMENTATION PERTINENTE TROUV√âE:\n'
        
        for (const result of vectorResults) {
          const metadata = result.metadata ? JSON.parse(result.metadata) : {}
          const title = metadata.title || `Document ${result.document_id}`
          const similarity = Math.round(result.similarity * 100)
          
          if (result.document_type === 'procedure') {
            foundProcedures.push(title)
            contextInfo += `\nüîß PROC√âDURE: "${title}" (pertinence: ${similarity}%)\n`
            contextInfo += `   ${result.content.slice(0, 500)}\n`
          } else if (result.document_type === 'tip') {
            foundTips.push(title)
            contextInfo += `\nüí° TIP: "${title}" (pertinence: ${similarity}%)\n`
            contextInfo += `   ${result.content.slice(0, 300)}\n`
          }
        }
      }
    } catch (error) {
      console.warn('Recherche vectorielle non disponible:', error)
    }

    // Recherche par mots-cl√©s en fallback si pas de r√©sultats vectoriels
    if (!contextInfo) {
      try {
        // Recherche dans les proc√©dures
        const keywords = message.toLowerCase().split(/\s+/).filter((w: string) => w.length > 3).slice(0, 5)
        if (keywords.length > 0) {
          const procedures = await db.procedure.findMany({
            where: {
              OR: keywords.map((keyword: string) => ({
                OR: [
                  { title: { contains: keyword, mode: 'insensitive' as const } },
                  { description: { contains: keyword, mode: 'insensitive' as const } },
                  { tags: { contains: keyword, mode: 'insensitive' as const } },
                ]
              }))
            },
            include: { steps: { orderBy: { order: 'asc' } } },
            take: 3
          })

          if (procedures.length > 0) {
            contextInfo += '\nüìñ PROC√âDURES LI√âES (recherche par mots-cl√©s):\n'
            for (const proc of procedures) {
              foundProcedures.push(proc.title)
              contextInfo += `\nüîß "${proc.title}"\n`
              contextInfo += `   ${proc.description || ''}\n`
              if (proc.steps.length > 0) {
                contextInfo += `   √âtapes: ${proc.steps.map(s => s.title).join(' ‚Üí ')}\n`
              }
            }
          }

          // Recherche dans les tips
          const tips = await db.tip.findMany({
            where: {
              OR: keywords.map((keyword: string) => ({
                OR: [
                  { title: { contains: keyword, mode: 'insensitive' as const } },
                  { content: { contains: keyword, mode: 'insensitive' as const } },
                  { tags: { contains: keyword, mode: 'insensitive' as const } },
                ]
              }))
            },
            take: 3
          })

          if (tips.length > 0) {
            contextInfo += '\nüí° TIPS LI√âS:\n'
            for (const tip of tips) {
              foundTips.push(tip.title)
              contextInfo += `\n"${tip.title}" (${tip.category || 'G√©n√©ral'})\n`
              contextInfo += `   ${tip.content.slice(0, 200)}...\n`
            }
          }
        }
      } catch (error) {
        console.warn('Recherche par mots-cl√©s √©chou√©e:', error)
      }
    }

    // Ajouter le contexte de la proc√©dure si disponible
    if (context?.procedure_id) {
      const procedure = await db.procedure.findUnique({
        where: { id: context.procedure_id },
        include: {
          steps: {
            orderBy: { order: 'asc' },
          },
        },
      })

      if (procedure) {
        contextInfo += `\n\nüìã CONTEXTE PROC√âDURE EN COURS: "${procedure.title}"\n`
        contextInfo += `Description: ${procedure.description || 'Non sp√©cifi√©e'}\n`
        contextInfo += `√âtapes:\n${procedure.steps.map((s, i) => `  ${i + 1}. ${s.title}: ${s.description || s.instructions || ''}`).join('\n')}\n`
      }
    }

    // Ajouter r√©sum√© des r√©f√©rences trouv√©es
    if (foundProcedures.length > 0 || foundTips.length > 0) {
      contextInfo += `\nüìö R√âF√âRENCES DISPONIBLES:\n`
      if (foundProcedures.length > 0) {
        contextInfo += `- Proc√©dures: ${foundProcedures.join(', ')}\n`
      }
      if (foundTips.length > 0) {
        contextInfo += `- Tips: ${foundTips.join(', ')}\n`
      }
    } else {
      contextInfo += '\n‚ö†Ô∏è Aucune documentation sp√©cifique trouv√©e dans la base. Utilise tes connaissances d\'expert.\n'
    }

    // Construire le message syst√®me avec le contexte
    const systemMessage = EXPERT_SYSTEM_PROMPT.replace('{context}', contextInfo)

    // Construire les messages pour OpenAI
    const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
      {
        role: 'system',
        content: systemMessage,
      },
      ...history
        .reverse()
        .filter((msg) => msg.message && msg.response)
        .flatMap((msg) => [
          {
            role: 'user' as const,
            content: msg.message,
          },
          {
            role: 'assistant' as const,
            content: msg.response!,
          },
        ]),
      {
        role: 'user',
        content: message,
      },
    ]

    // Sauvegarder le message utilisateur
    const chatMessage = await db.chatMessage.create({
      data: {
        userId: user.id,
        message,
        context: context ? JSON.stringify(context) : null,
      },
    })

    // Appeler OpenAI avec streaming (temp√©rature l√©g√®rement r√©duite pour plus de pr√©cision)
    const openai = getOpenAIClient()
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages,
      stream: true,
      temperature: 0.5,
      max_tokens: 2000,
    })

    // Cr√©er un stream de r√©ponse
    const stream = new ReadableStream({
      async start(controller) {
        let fullResponse = ''

        try {
          for await (const chunk of completion) {
            const content = chunk.choices[0]?.delta?.content || ''
            if (content) {
              fullResponse += content
              controller.enqueue(
                new TextEncoder().encode(`data: ${JSON.stringify({ content })}\n\n`)
              )
            }
          }

          controller.enqueue(new TextEncoder().encode('data: [DONE]\n\n'))

          // Sauvegarder la r√©ponse compl√®te
          await db.chatMessage.update({
            where: { id: chatMessage.id },
            data: { response: fullResponse },
          })

          controller.close()
        } catch (error) {
          console.error('Erreur streaming:', error)
          controller.enqueue(
            new TextEncoder().encode(
              `data: ${JSON.stringify({ error: 'Erreur lors de la g√©n√©ration' })}\n\n`
            )
          )
          controller.close()
        }
      },
    })

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        Connection: 'keep-alive',
      },
    })
  } catch (error) {
    console.error('Erreur chat:', error)
    return new Response('Erreur serveur', { status: 500 })
  }
}

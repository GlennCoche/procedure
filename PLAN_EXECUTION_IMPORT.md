# Plan d'Ex√©cution - Import de Documentation Technique

Ce plan d√©taille les √©tapes pour installer, configurer et ex√©cuter le syst√®me d'import de documentation.

## üìã Vue d'ensemble

**Objectif** : Importer automatiquement les 763 fichiers techniques du dossier `docs/` dans la base de donn√©es pour alimenter les proc√©dures, tips et enrichir le Chat IA.

**Dur√©e estim√©e** : 2-3 heures (selon le nombre de documents)

---

## √âtape 1 : Installation des D√©pendances Python

### 1.1 V√©rifier l'environnement Python

```bash
# V√©rifier la version Python (3.8+ requis)
python3 --version

# V√©rifier si pip est install√©
python3 -m pip --version
```

### 1.2 Activer l'environnement virtuel (si existant)

```bash
cd /Users/glenn/Desktop/procedures/backend

# Si un venv existe d√©j√†
source venv/bin/activate  # Sur macOS/Linux
# OU
venv\Scripts\activate  # Sur Windows
```

### 1.3 Installer les d√©pendances

```bash
# Depuis le dossier backend
cd /Users/glenn/Desktop/procedures/backend

# Installer toutes les d√©pendances
pip install -r requirements.txt

# V√©rifier l'installation
pip list | grep -E "(pdfplumber|pymupdf|openai|pgvector|tqdm)"
```

**D√©pendances √† installer** :
- `pdfplumber==0.10.3` - Extraction PDF
- `pymupdf==1.23.8` - Alternative extraction PDF
- `python-docx==1.1.0` - Extraction DOCX
- `pgvector==0.2.4` - Support vectoriel PostgreSQL
- `psycopg2-binary==2.9.9` - Driver PostgreSQL
- `sentence-transformers==2.3.1` - Embeddings locaux (optionnel)
- `tqdm==4.66.1` - Barres de progression

**V√©rification** :
```bash
python3 -c "import pdfplumber; import fitz; import openai; print('‚úÖ Toutes les d√©pendances sont install√©es')"
```

---

## √âtape 2 : Appliquer la Migration SQL sur Supabase

### 2.1 Acc√©der √† Supabase SQL Editor

1. Aller sur : https://supabase.com/dashboard/project/[VOTRE_PROJECT_ID]/sql/new
2. Ou via : Dashboard ‚Üí SQL Editor ‚Üí New Query

### 2.2 Copier et ex√©cuter la migration

Copier le contenu du fichier `frontend/prisma/migrations/1_add_document_embeddings/migration.sql` :

```sql
-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Create document_embeddings table
CREATE TABLE IF NOT EXISTS "document_embeddings" (
    "id" SERIAL PRIMARY KEY,
    "document_type" VARCHAR(50) NOT NULL,
    "document_id" INTEGER NOT NULL,
    "content" TEXT NOT NULL,
    "embedding" vector(1536),
    "metadata" TEXT,
    "created_at" TIMESTAMPTZ(6) NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Create indexes for efficient queries
CREATE INDEX IF NOT EXISTS "document_embeddings_document_type_document_id_idx" 
ON "document_embeddings"("document_type", "document_id");

CREATE INDEX IF NOT EXISTS "document_embeddings_document_type_idx" 
ON "document_embeddings"("document_type");

-- Create vector index for similarity search (using HNSW for better performance)
CREATE INDEX IF NOT EXISTS "document_embeddings_embedding_idx" 
ON "document_embeddings" 
USING hnsw (embedding vector_cosine_ops);
```

### 2.3 V√©rifier l'application

Ex√©cuter cette requ√™te pour v√©rifier :

```sql
-- V√©rifier que l'extension est activ√©e
SELECT * FROM pg_extension WHERE extname = 'vector';

-- V√©rifier que la table existe
SELECT table_name 
FROM information_schema.tables 
WHERE table_name = 'document_embeddings';

-- V√©rifier les index
SELECT indexname, indexdef 
FROM pg_indexes 
WHERE tablename = 'document_embeddings';
```

**R√©sultat attendu** :
- Extension `vector` activ√©e
- Table `document_embeddings` cr√©√©e
- 3 index cr√©√©s (dont l'index vectoriel HNSW)

### 2.4 Alternative : Via Prisma Migrate (si pr√©f√©r√©)

```bash
cd /Users/glenn/Desktop/procedures/frontend

# S'assurer que DATABASE_URL est configur√© dans .env.local
# Puis appliquer la migration
npx prisma migrate deploy
```

---

## √âtape 3 : Lancer l'Inventaire des Documents

### 3.1 Ex√©cuter le script d'inventaire

```bash
cd /Users/glenn/Desktop/procedures

# Lancer l'inventaire
python3 scripts/inventory_docs.py
```

### 3.2 V√©rifier les r√©sultats

Le script g√©n√®re 3 fichiers dans `scripts/inventory_output/` :

```bash
# Voir le r√©sum√©
cat scripts/inventory_output/summary.json | python3 -m json.tool

# Voir les statistiques par marque
python3 -c "
import json
with open('scripts/inventory_output/summary.json') as f:
    data = json.load(f)
    print('üìä R√©sum√© par marque:')
    for brand, info in data['by_brand'].items():
        print(f'  {brand}: {info[\"count\"]} fichiers ({info[\"size_mb\"]} MB)')
"
```

**R√©sultat attendu** :
- ABB : ~11 fichiers
- Delta : ~701 fichiers
- Goodwe : ~20 fichiers
- Huawei : ~19 fichiers
- Sungrow : ~2 fichiers
- Webdynsun : ~4 fichiers
- WebdynsunPM : ~4 fichiers
- Bridage Raccordement : ~2 fichiers

---

## √âtape 4 : Importer une Marque (Test avec ABB)

### 4.1 Pr√©requis

- Backend API d√©marr√© sur `http://localhost:8000`
- Utilisateur admin cr√©√© (email et mot de passe requis)

### 4.2 V√©rifier que le backend est d√©marr√©

```bash
# Tester la connexion API
curl http://localhost:8000/api/health

# Ou ouvrir dans le navigateur
open http://localhost:8000/docs
```

### 4.3 Importer la marque ABB

```bash
cd /Users/glenn/Desktop/procedures

# Importer ABB (11 PDFs - bon pour tester)
python3 scripts/import_documents.py \
  --brand ABB \
  --api-url http://localhost:8000 \
  --email admin@procedures.local \
  --password admin123
```

**Remplacez** :
- `admin@procedures.local` par votre email admin
- `admin123` par votre mot de passe admin

### 4.4 V√©rifier l'import

```bash
# V√©rifier via l'API
curl http://localhost:8000/api/procedures?category=ABB

# Ou compter les proc√©dures cr√©√©es
curl http://localhost:8000/api/procedures | python3 -c "
import sys, json
data = json.load(sys.stdin)
abb_procs = [p for p in data if 'ABB' in p.get('category', '')]
print(f'‚úÖ {len(abb_procs)} proc√©dures ABB cr√©√©es')
"
```

### 4.5 Importer d'autres marques (optionnel)

```bash
# Importer toutes les marques (long processus)
python3 scripts/import_documents.py \
  --all \
  --api-url http://localhost:8000 \
  --email admin@procedures.local \
  --password admin123
```

**Ordre recommand√©** :
1. ABB (11 PDFs) - Test initial ‚úÖ
2. Huawei (19 fichiers)
3. Goodwe (20 PDFs)
4. Sungrow (2 PDFs)
5. Webdynsun/WebdynsunPM (8 fichiers)
6. Delta (701 fichiers) - Le plus volumineux
7. Bridage Raccordement (2 PDFs)

---

## √âtape 5 : G√©n√©rer les Embeddings

### 5.1 Pr√©requis

- Cl√© API OpenAI configur√©e
- Proc√©dures et tips import√©s dans la base de donn√©es

### 5.2 Configurer la cl√© OpenAI

**Option A : Variable d'environnement (recommand√©)**

```bash
export OPENAI_API_KEY="sk-..."
```

**Option B : Argument de ligne de commande**

```bash
python3 scripts/generate_embeddings.py --openai-key "sk-..."
```

### 5.3 G√©n√©rer les embeddings

```bash
cd /Users/glenn/Desktop/procedures

# G√©n√©rer pour toutes les proc√©dures et tips
python3 scripts/generate_embeddings.py \
  --api-url http://localhost:8000 \
  --openai-key $OPENAI_API_KEY

# OU si la variable d'environnement est configur√©e
python3 scripts/generate_embeddings.py --api-url http://localhost:8000
```

**Options disponibles** :
```bash
# G√©n√©rer uniquement pour les proc√©dures
python3 scripts/generate_embeddings.py --procedures-only

# G√©n√©rer uniquement pour les tips
python3 scripts/generate_embeddings.py --tips-only

# Limiter le nombre (pour test)
python3 scripts/generate_embeddings.py --limit 10
```

### 5.4 V√©rifier la g√©n√©ration

```bash
# V√©rifier dans Supabase SQL Editor
SELECT 
  document_type,
  COUNT(*) as count,
  COUNT(embedding) as with_embedding
FROM document_embeddings
GROUP BY document_type;
```

**R√©sultat attendu** :
- `procedure` : X embeddings g√©n√©r√©s
- `tip` : Y embeddings g√©n√©r√©s

### 5.5 Import manuel des embeddings (si n√©cessaire)

Si le script sauvegarde dans `scripts/embeddings_output/`, vous pouvez les importer manuellement :

```sql
-- Exemple d'import depuis un fichier JSON
-- (Adapter selon le format g√©n√©r√©)
INSERT INTO document_embeddings (document_type, document_id, content, embedding, metadata)
VALUES (
  'procedure',
  1,
  'Contenu...',
  '[0.1, 0.2, ...]'::vector,
  '{"title": "..."}'::jsonb
);
```

---

## V√©rification Finale

### Tester le Chat IA enrichi

1. D√©marrer le frontend : `cd frontend && npm run dev`
2. Ouvrir : http://localhost:3000
3. Se connecter
4. Tester le Chat IA avec une question technique
5. V√©rifier que le contexte enrichi est utilis√©

### V√©rifier la recherche vectorielle

```bash
# Tester via l'API (si endpoint cr√©√©)
curl -X POST http://localhost:8000/api/vector-search \
  -H "Content-Type: application/json" \
  -d '{"query": "configuration onduleur ABB"}'
```

---

## D√©pannage

### Erreur : Module non trouv√©

```bash
# R√©installer les d√©pendances
pip install --upgrade -r backend/requirements.txt
```

### Erreur : Extension vector non disponible

```sql
-- V√©rifier dans Supabase
SELECT * FROM pg_available_extensions WHERE name = 'vector';

-- Si non disponible, contacter le support Supabase
```

### Erreur : Authentification API √©chou√©e

- V√©rifier que le backend est d√©marr√©
- V√©rifier les identifiants admin
- V√©rifier que l'utilisateur a le r√¥le "admin"

### Erreur : Limite de taux OpenAI

- Attendre quelques minutes
- Utiliser `--limit` pour traiter par petits lots
- V√©rifier les quotas sur https://platform.openai.com/usage

---

## Prochaines √âtapes

Une fois l'import termin√© :

1. ‚úÖ V√©rifier la qualit√© des proc√©dures cr√©√©es
2. ‚úÖ Tester le Chat IA avec diff√©rentes questions
3. ‚úÖ Ajuster les param√®tres de recherche vectorielle si n√©cessaire
4. ‚úÖ Ajouter plus de documents si besoin
5. ‚úÖ Monitorer l'utilisation et les performances

---

## Notes Importantes

- **Co√ªts OpenAI** : La g√©n√©ration d'embeddings utilise `text-embedding-3-small` (mod√®le √©conomique)
- **Performance** : L'index HNSW permet des recherches rapides m√™me avec beaucoup de documents
- **Doublons** : Le syst√®me d√©tecte automatiquement les fichiers d√©j√† trait√©s (hash MD5)
- **Logs** : Tous les scripts affichent des logs d√©taill√©s pour le suivi

---

## Support

En cas de probl√®me :
1. V√©rifier les logs des scripts
2. V√©rifier les logs du backend (console)
3. V√©rifier les logs Supabase (Dashboard ‚Üí Logs)
4. Consulter `scripts/README_IMPORT_DOCS.md` pour plus de d√©tails
